{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prepared_data.csv - after deleting short sentences, unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word vectors. 400000 соответствий слово -> число"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = np.load('wordsList.npy')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "raw_data = pd.read_csv(\"text_emotion.csv\")\n",
    "keep_col = ['sentiment','content']\n",
    "df = raw_data[keep_col]\n",
    "# print(\"Raw data head:\", df.head(100))\n",
    "# print(\"Number of tweets grouped by sentiment:\", df.groupby(['sentiment']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split familiar categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets grouped by sentiment:            content\n",
      "sentiment         \n",
      "anger         1433\n",
      "fun           9270\n",
      "love          3842\n",
      "neutral       8638\n",
      "sadness       6171\n",
      "surprise      2187\n",
      "worry         8459\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if (row[\"sentiment\"] == \"hate\"):\n",
    "        row[\"sentiment\"] = \"anger\"\n",
    "    if (row[\"sentiment\"] == \"boredom\" or row[\"sentiment\"] == \"empty\"):\n",
    "        row[\"sentiment\"] = \"sadness\"\n",
    "    if (row[\"sentiment\"] == \"happiness\" or row[\"sentiment\"] == \"relief\" or row[\"sentiment\"] == \"enthusiasm\"):\n",
    "        row[\"sentiment\"] = \"fun\"\n",
    "        \n",
    "print(\"Number of tweets grouped by sentiment:\", df.groupby(['sentiment']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    string = re.sub(r'@\\w+','',string)                                      # Delete @* references\n",
    "    string = re.sub(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+',' ', string) # Delete http(s):// links\n",
    "    string = re.sub(r'\\.+',' ', string)                                     # Replace ... with ' '\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    row[\"content\"] = cleanSentences(row[\"content\"])\n",
    "    \n",
    "# print(\"Raw data head:\", df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence length hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numWords = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    numWords.append(len(row[\"content\"].split()))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 28)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([5, 36, 0, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление из датасета всех предложений, где есть неизвестные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentin/anaconda3/envs/tensorflow-v1.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown words: 12870\n",
      "Number of tweets grouped by sentiment:            content\n",
      "sentiment         \n",
      "anger          920\n",
      "fun           5945\n",
      "love          2466\n",
      "neutral       5980\n",
      "sadness       4376\n",
      "surprise      1446\n",
      "worry         5997\n"
     ]
    }
   ],
   "source": [
    "unknownWordCounter = 0\n",
    "for index, row in df.iterrows():\n",
    "    split = row[\"content\"].split()\n",
    "    for word in split:\n",
    "            if word not in wordsList:\n",
    "                unknownWordCounter = unknownWordCounter + 1\n",
    "                df.drop(index, inplace=True)\n",
    "                break\n",
    "print(\"Number of unknown words:\", unknownWordCounter)\n",
    "\n",
    "print(\"Number of tweets grouped by sentiment:\", df.groupby(['sentiment']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление предложений с количеством слов < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentin/anaconda3/envs/tensorflow-v1.6/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets grouped by sentiment:            content\n",
      "sentiment         \n",
      "anger          827\n",
      "fun           5266\n",
      "love          2115\n",
      "neutral       4527\n",
      "sadness       3826\n",
      "surprise      1259\n",
      "worry         5413\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    split = row[\"content\"].split()\n",
    "    if len(split) < 5:\n",
    "        df.drop(index, inplace=True)\n",
    "print(\"Number of tweets grouped by sentiment:\", df.groupby(['sentiment']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение готовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d388c25af50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prepared_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.head(100))\n",
    "df.to_csv('prepared_data.csv', sep='\\t', encoding='utf-8')\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grouped grouped_data.csv with 800 sentence instances of each sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23233\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"prepared_data.csv\", sep='\\t')\n",
    "keep_col = ['sentiment','content']\n",
    "df = df[keep_col]\n",
    "print(len(df.index))\n",
    "grouped = df.groupby('sentiment')\n",
    "anger_group = grouped.get_group('anger')\n",
    "fun_group = grouped.get_group('fun')\n",
    "love_group = grouped.get_group('love')\n",
    "sadness_group = grouped.get_group('sadness')\n",
    "surprise_group = grouped.get_group('surprise')\n",
    "\n",
    "def getGroupHead(DataFrame):\n",
    "    return DataFrame.head(800)\n",
    "\n",
    "anger_group = getGroupHead(anger_group)\n",
    "fun_group = getGroupHead(fun_group)\n",
    "love_group = getGroupHead(love_group)\n",
    "sadness_group = getGroupHead(sadness_group)\n",
    "surprise_group = getGroupHead(surprise_group)\n",
    "\n",
    "frames = [anger_group, fun_group, love_group, sadness_group, surprise_group]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "type(anger_group)\n",
    "print(len(df.index))\n",
    "\n",
    "df.to_csv('grouped_data.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование предложений в вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    20     14    100 ...      0      0      0]\n",
      " [109542  45120  10946 ...      0      0      0]\n",
      " [    37   2557    646 ...      0      0      0]\n",
      " ...\n",
      " [    54     81    733 ...      0      0      0]\n",
      " [ 28125     33      4 ...      0      0      0]\n",
      " [    41    180      4 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "numFiles = 4000\n",
    "maxSeqLength = 30\n",
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for index, row in df.iterrows():\n",
    "    indexCounter = 0\n",
    "    split = row[\"content\"].split()\n",
    "    for word in split:\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "    fileCounter = fileCounter + 1\n",
    "    \n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    20     14    100  17249     61     67   2383  22392     13     71\n",
      "    951      6 201534    699      3 201534    364      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "[    41    180      4     30  79289    751      7    306    494   1052\n",
      "      6 261038    839  14663      7    202    941      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "print(ids[0])\n",
    "print(ids[3999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции получение batch для тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        r = randint(1,699)\n",
    "        if (i % 5 == 0):\n",
    "            num = r\n",
    "            labels.append([1,0,0,0,0])\n",
    "        if (i % 5 == 1): \n",
    "            num = 800 + r\n",
    "            labels.append([0,1,0,0,0])\n",
    "        if (i % 5 == 2): \n",
    "            num = 1600 + r\n",
    "            labels.append([0,0,1,0,0])\n",
    "        if (i % 5 == 3): \n",
    "            num = 2400 + r\n",
    "            labels.append([0,0,0,1,0])\n",
    "        if (i % 5 == 4): \n",
    "            num = 3200 + r\n",
    "            labels.append([0,0,0,0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        r = randint(699,799)\n",
    "        if (i % 6 == 0):\n",
    "            num = r\n",
    "            labels.append([1,0,0,0,0])\n",
    "        if (i % 6 == 1): \n",
    "            num = 800 + r\n",
    "            labels.append([0,1,0,0,0])\n",
    "        if (i % 6 == 2): \n",
    "            num = 1600 + r\n",
    "            labels.append([0,0,1,0,0])\n",
    "        if (i % 6 == 3): \n",
    "            num = 2400 + r\n",
    "            labels.append([0,0,0,1,0])\n",
    "        if (i % 6 == 4): \n",
    "            num = 3200 + r\n",
    "            labels.append([0,0,0,0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-4b6cf3708d62>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batchSize = 28\n",
    "lstmUnits = 64\n",
    "numClasses = 5\n",
    "iterations = 100000\n",
    "numDimensions = 300\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight, name=\"lstm_prediction\") + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "sess = tf.InteractiveSession()\n",
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/five_dimensions.ckpt-10000\n",
      "saved to models/five_dimensions.ckpt-20000\n",
      "saved to models/five_dimensions.ckpt-30000\n",
      "saved to models/five_dimensions.ckpt-40000\n",
      "saved to models/five_dimensions.ckpt-50000\n",
      "saved to models/five_dimensions.ckpt-60000\n",
      "saved to models/five_dimensions.ckpt-70000\n",
      "saved to models/five_dimensions.ckpt-80000\n",
      "saved to models/five_dimensions.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/five_dimensions.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/five_dimensions.ckpt-90000\n",
      "[12.183834  -1.6347973 -1.3660951 -1.1432288 -1.7402791]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix\n",
    "\n",
    "\n",
    "inputText = \"That movie was terrible.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)\n",
    "\n",
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "\n",
    "print(predictedSentiment)\n",
    "#anger, fun, love, sadness, surprise\n",
    "\n",
    "\n",
    "# This was the worst thing ever happened with me --sandess\n",
    "# I have really a nice time today playing football --love\n",
    "# I like autumn evenings for it's atmosphere --love\n",
    "# I love you --love\n",
    "# I hate you --anger\n",
    "# Have a nice day! --love\n",
    "# I hope you die --sadness\n",
    "# I never was so tired like now, I want this all go to end --sadness\n",
    "# Wow, i never expect the elephant is too big --surprise\n",
    "# Let's go party! --surprise\n",
    "# Omg, are you kidding me? --surprise\n",
    "# I lose all my money today on Poker --fun\n",
    "# The sky so heavy --sadness\n",
    "# When I this about future of Russia it's getting cold in my soul --surprise\n",
    "# Wash it all away --surprise\n",
    "# Wake me up, when september ends --sadness\n",
    "# We are looking for you to start up a fight --surprise\n",
    "# There's no money there's no possessions Only obsession I don't need that shit Take my money take my obsession --anger --surprise\n",
    "\n",
    "\n",
    "# https://www.fluentu.com/blog/english/advanced-english-phrases/\n",
    "# She’s flying high after the successful product launch. --fun\n",
    "# He’s pumped up for his first half-marathon race this weekend. --love\n",
    "# I always feel down in the dumps when I go back to work after a long weekend. --sadness\n",
    "# I just asked one question to confirm his request, and my boss bit my head off. --anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.858811  14.350354  -4.1434903 -2.6799266 -2.3185062]\n"
     ]
    }
   ],
   "source": [
    "inputText = \"I have really a nice time today playing football\"\n",
    "inputMatrix = getSentenceMatrix(inputText)\n",
    "\n",
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "\n",
    "print(predictedSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Были убраны некоторые категории, некоторые склеены. Проведена подготовка данных, убраны лишние знаки препинания, ссылки, данные преобразованы в целочисленный вектор значений. Удалены слишком короткие предложения и предложения с неизвестными словами"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow1.6",
   "language": "python",
   "name": "tensorflow-v1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
